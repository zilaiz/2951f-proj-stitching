{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ecc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid_exp.env import SimpleEnv\n",
    "from minigrid_exp.data_generation import data_gen\n",
    "import os\n",
    "\n",
    "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "\n",
    "env = SimpleEnv(size=11, agent_start_pos=(1, 1), agent_start_dir=0, render_mode='rgb_array', max_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa8d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "obs, _ = env.reset()\n",
    "print(env.agent_pos)\n",
    "print(env.agent_dir)\n",
    "print(env.goal_pos)\n",
    "display(Image.fromarray(env.render()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6d237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# exp_trajs = data_gen(env, total_ep=2400)\n",
    "# rand_trajs = data_gen(env, total_ep=2400, is_random=True)\n",
    "# with open('./minigrid_exp/expert_minigrid_trajs.pkl', 'wb') as fp:\n",
    "#     pickle.dump(exp_trajs, fp)\n",
    "with open('./minigrid_exp/expert_minigrid_trajs.pkl', 'rb') as fp:\n",
    "    exp_trajs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(exp_trajs['observations']))\n",
    "print(len(exp_trajs['actions']))\n",
    "print(len(exp_trajs['dones']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "def eval(env, dataset, policy, test_starting_pos, test_goal_pos, save_dir, is_save=False, is_random=False):\n",
    "    policy.eval()\n",
    "    success = []\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for i, (start, goal) in enumerate(zip(test_starting_pos, test_goal_pos)):\n",
    "        env.reset(goal_pos = goal)\n",
    "        env.agent_pos = start[:2]\n",
    "        env.agent_dir = start[2]\n",
    "        reward = 0.\n",
    "        imgs = [Image.fromarray(env.render())]\n",
    "        \n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            x, y = env.agent_pos\n",
    "            d = env.agent_dir\n",
    "            agent_obs = torch.tensor(dataset.obs_to_one_hot([x, y, d])).long().to('cuda')\n",
    "            # a = policy(agent_obs, torch.tensor(goal, device='cuda')[None]).argmax(dim=-1)\n",
    "            # a = policy(agent_obs, None)\n",
    "            if is_random:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                dist = Categorical(logits=policy(agent_obs, None))\n",
    "                a = dist.sample().cpu().numpy()\n",
    "            _, _, terminated, truncated, _ = env.step(a)\n",
    "            imgs.append(Image.fromarray(env.render()))\n",
    "            if terminated or truncated:\n",
    "                reward = float(np.linalg.norm(np.array(env.agent_pos) - np.array(goal)) == 0)\n",
    "                break\n",
    "        \n",
    "        success.append(reward)\n",
    "        if is_save:\n",
    "            img_one = imgs[0]\n",
    "            img_one.save(f'{save_dir}/start:{start}_goal:{goal}_reward:{reward}.gif', format='GIF', append_images=imgs[1:],\n",
    "               save_all=True, duration=100, loop=0)\n",
    "    \n",
    "    success = torch.tensor(success)\n",
    "    policy.train()\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "# test_starting_pos = [[5,5,3], [5,5,3], [5,5,3], [5,5,3]]\n",
    "# test_goal_pos = [[2, 2], [8, 8], [3, 9], [7,1]]\n",
    "test_starting_pos = [[1, 1, 0], [1, 1, 1], [2, 2, 2], [6, 7, 1], [6, 6, 0], [2, 1, 1], [8, 4, 3], [7, 1, 0], [9, 7, 1], [7, 3, 2], [9, 4, 1], [8, 8, 1]]\n",
    "test_goal_pos = [[9, 1], [9, 4], [7, 8], [1, 3], [3, 7], [1, 8], [4, 1], [9, 8], [1, 3], [2, 7], [7, 9], [4, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c92a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigrid_exp.policies import BCPolicy\n",
    "from minigrid_exp.minigrid_dataset import MiniGridDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "traj_dataset = MiniGridDataset(env, exp_trajs)\n",
    "traj_dataset.update_data()\n",
    "traj_dataloader = DataLoader(traj_dataset, batch_size=64, shuffle=True)\n",
    "bc_policy = BCPolicy(len(traj_dataset.all_unique_obs), 256)\n",
    "bc_policy.to('cuda:0')\n",
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(bc_policy.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = int(1e4)\n",
    "eval_freq = 100\n",
    "bc_sr_list = []\n",
    "\n",
    "save_folder = 'video/gcbc'\n",
    "\n",
    "if os.path.exists(save_folder):\n",
    "    shutil.rmtree(save_folder)\n",
    "\n",
    "with tqdm(total=total_steps) as pbar:\n",
    "    for step in range(total_steps):\n",
    "        batch = next(iter(traj_dataloader))\n",
    "        observations, actions, rewards, next_observations, _, masks, oh_actions = batch\n",
    "        pred_logits = bc_policy(observations.to('cuda:0'), None)\n",
    "        loss = CEloss(pred_logits, actions.to('cuda:0').view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "        pbar.update(1)\n",
    "\n",
    "        if step > 9000 and (step + 1) % eval_freq == 0:\n",
    "            with torch.no_grad():\n",
    "                success = eval(env, traj_dataset, bc_policy, test_starting_pos, test_goal_pos, f'{save_folder}/eval_{(step + 1) // eval_freq}', is_save=True)\n",
    "                sr = success.mean().item()\n",
    "            bc_sr_list.append(sr)\n",
    "\n",
    "print(bc_sr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from torch.distributions import Categorical\n",
    "from minigrid_exp.policies import IQL\n",
    "\n",
    "save_folder = 'video/gciql'\n",
    "\n",
    "if os.path.exists(save_folder):\n",
    "    shutil.rmtree(save_folder)\n",
    "\n",
    "def expectile_loss(pred, target, expectile=0.5):\n",
    "    diff = pred - target\n",
    "    weight = torch.where(diff > 0, expectile, (1 - expectile))\n",
    "    return weight * (diff ** 2)\n",
    "\n",
    "def one_hot_action(a: int, n_actions: int):\n",
    "    return torch.eye(n_actions)[a]\n",
    "\n",
    "iql_sr_dict = {}\n",
    "\n",
    "for starting_pos, eval_goal in zip(test_starting_pos[:2], test_goal_pos[:2]):\n",
    "    traj_dataset.update_data(eval_goal=eval_goal)\n",
    "    iql_sr_dict[tuple(eval_goal)] = []\n",
    "    iql_policy = IQL(len(traj_dataset.all_unique_obs), 256, 4, 0.005).to('cuda')\n",
    "    gamma = 0.99\n",
    "    expectile = 0.9\n",
    "    temp = 10.0\n",
    "    clip_score = 100.0\n",
    "    target_update_freq = 1\n",
    "    total_steps = int(1e4)\n",
    "\n",
    "    actor_optim = torch.optim.Adam(itertools.chain(iql_policy.actor.parameters(), iql_policy.obs_enc.parameters()), lr=3e-4)\n",
    "    critic_optim = torch.optim.Adam(iql_policy.critic.parameters(), lr=3e-4)\n",
    "    value_optim = torch.optim.Adam(iql_policy.value.parameters(), lr=3e-4)\n",
    "\n",
    "    with tqdm(total=total_steps) as pbar:\n",
    "        for step in range(total_steps):\n",
    "            batch = next(iter(traj_dataloader))\n",
    "            observations, actions, rewards, next_observations, _, masks, oh_actions = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_obs_emb = iql_policy.obs_enc(next_observations.to('cuda'), None)\n",
    "            obs_emb = iql_policy.obs_enc(observations.to('cuda'), None)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                target_q1, target_q2 = iql_policy.target_critic_forward(obs_emb, oh_actions.to('cuda'))\n",
    "                target_q_min = torch.cat([target_q1, target_q2], dim=-1).min(dim=-1).values\n",
    "            \n",
    "            v = iql_policy.value_forward(obs_emb.detach())\n",
    "            value_loss = expectile_loss(target_q_min, v, expectile=expectile).mean(dim=-1).sum()\n",
    "\n",
    "            value_optim.zero_grad(set_to_none=True)\n",
    "            value_loss.backward()\n",
    "            value_optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_v = iql_policy.value_forward(next_obs_emb)\n",
    "                target = rewards.to('cuda') + gamma * masks.to('cuda') * next_v\n",
    "            q1, q2 = iql_policy.critic_forward(obs_emb.detach(), oh_actions.to('cuda'))\n",
    "            critic_loss = ((target - q1) ** 2 + (target - q2) ** 2).mean(dim=-1).sum()\n",
    "\n",
    "            critic_optim.zero_grad(set_to_none=True)\n",
    "            critic_loss.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                adv = target_q_min - v.view(-1)\n",
    "                exp_adv = torch.exp(temp * adv)\n",
    "                if clip_score is not None:\n",
    "                    exp_adv = torch.clamp(exp_adv, max=clip_score)\n",
    "            dist = Categorical(logits=iql_policy.actor_forward(obs_emb))\n",
    "            actor_loss = (exp_adv * -dist.log_prob(actions.to('cuda').view(-1))).sum(dim=-1)\n",
    "\n",
    "            actor_optim.zero_grad(set_to_none=True)\n",
    "            actor_loss.backward()\n",
    "            actor_optim.step()\n",
    "\n",
    "            if step % target_update_freq == 0:  # update target\n",
    "                iql_policy.target_update()\n",
    "\n",
    "            # actor_optim.zero_grad(set_to_none=True)\n",
    "            # critic_optim.zero_grad(set_to_none=True)\n",
    "            # value_optim.zero_grad(set_to_none=True)\n",
    "            # (actor_loss + value_loss + critic_loss).backward()\n",
    "            # actor_optim.step()\n",
    "            # critic_optim.step()\n",
    "            # value_optim.step()\n",
    "            pbar.set_postfix({'value_loss': value_loss.item(), 'critic_loss': critic_loss.item(), 'actor_loss': actor_loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "            if step > 9000 and (step + 1) % eval_freq == 0:\n",
    "                with torch.no_grad():\n",
    "                    success = eval(env, traj_dataset, iql_policy, [starting_pos], [eval_goal], f'{save_folder}/eval_{(step + 1) // eval_freq}', is_save=True)\n",
    "                    sr = success.mean().item()\n",
    "                iql_sr_dict[tuple(eval_goal)].append(sr)\n",
    "\n",
    "print(iql_sr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaaed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observations = torch.tensor([traj_dataset.obs_to_one_hot([6, 4, 0])])\n",
    "# obs_emb = iql_policy.obs_enc(observations.to('cuda'), None)\n",
    "# action = torch.tensor([[0,0,1,0]]).to('cuda')\n",
    "# print(iql_policy.value_forward(obs_emb))\n",
    "# print(iql_policy.actor_forward(obs_emb))\n",
    "# print(iql_policy.critic_forward(obs_emb, action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py\n",
    "\n",
    "# filename = '/users/zzeng28/.d4rl/datasets/Ant_maze_hardest-maze_noisy_multistart_True_multigoal_True_sparse_fixed.hdf5'\n",
    "\n",
    "# with h5py.File(filename, 'r') as f:\n",
    "#     observations = f['observations'][()]\n",
    "#     terminals = np.array(f['terminals'][()])\n",
    "#     rewards = np.array(f['rewards'][()])\n",
    "#     print(np.argwhere(terminals == 1.).shape)\n",
    "#     print(np.argwhere(rewards == 1.).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
